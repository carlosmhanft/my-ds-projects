# -*- coding: utf-8 -*-
"""Pokemon_Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NaItyH3qeia5xLnxbFy2O1JhvaTDW9iO

<img src="//upload.wikimedia.org/wikipedia/commons/thumb/9/98/International_Pok%C3%A9mon_logo.svg/280px-International_Pok%C3%A9mon_logo.svg.png" height="150">

# **Regressão Logística: Identificando Pokémons Lendários**
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/pokemon.csv')

"""## Organizando base:

    1. Levantando alguns pontos encontrados (ex: valores nulos)
    2. Tratando estes valores nulos e textos incorretos
    3. Modificão na Coluna "type2" para ser igual a "type1" quando vazia
    4. etc.

"""

# Alguns valores nulos ocorreram por conta de algumas difulcades e limitações na extração desses dados
# onde os pokémons com nulos na altura e peso são aqueles que possuem diferentes formas (O caso dos Alolan)
# ou possuem diferentes formas físicas (Hoopa) ou diferenças entre o tipo do jogo (Lycanroc)
# vamos desconsiderar estes dados em uma modelagem futura.

# Fonte: https://www.kaggle.com/rounakbanik/pokemon/discussion/115718

df[df.weight_kg.isna()].loc[:,['name','weight_kg','height_m']]

#Ajustar o conteúdo da coluna classfication (24), tirando o termo Pokémon
df.classfication = df.classfication.replace('Pokémon','',regex=True)


# incluir valor na coluna type2 onde há valores nulos
df.type2 = np.where(df.type2.isna(),
                    df.type1,
                    df.type2)


# Corrigindo um valor da coluna que estava com texto estranho ao invés do valor 255
df.capture_rate = df.capture_rate.replace('30 (Meteorite)255 (Core)','255')


#Depois de ajustar a coluna, vamos transformar no tipo adequado de dado
df.capture_rate = df.capture_rate.astype('int32')


# Ajustar coluna de altura para valores nulos, colocar a média onde for nulo
df.height_m = np.where(df.height_m.isna(),
                              df.height_m.mean(),
                              df.height_m)


# Ajustar %masculino para valores nulos, colocar a média onde for nulo
df.percentage_male = np.where(df.percentage_male.isna(),
                                     df.percentage_male.mean(),
                                     df.percentage_male)


# Ajustar coluna de peso para valores nulos, colocar a média onde for nulo
df.weight_kg = np.where(df.weight_kg.isna(),
                               df.weight_kg.mean(),
                               df.weight_kg)

# separando as colunas por assunto, pra facilitar...
cols = df.columns.values

col_ability  = cols[0]
cols_against = cols[1:19]
cols_status  = cols[19:]

# Vamos colocar os nomes dos Pokémon como índice por serem valores únicos

df.set_index(df.name,inplace = True)

# Separando as colunas mais relevantes pra analisar
status = ['hp',
          'attack',
          'defense', 
          'speed',
          'sp_attack',
          'sp_defense',
          'type1',
          'type2',
          'capture_rate',
          'height_m',
          'weight_kg',
          'classfication',
          'generation',
          'is_legendary']

df_status = df[status]

"""# Preparação da base (Treino e Teste)"""

# 1. Antes de rodar os modelos de aprendizagem supervisionada (Classificação), precisamos separar a base em em 2 partes: Treino e Teste

# 2. Há várias modalidades de fazer esta separação mas vamos começar com a mais simples: "Hold-out" ou "Train-Test-Split"

# 3. Feito isso, vamos criar uma variável que irá computar todas as métricas dos modelos para verificarmos quem apresentou o melhor resultado

# Separar as variáveis numéricas das categóricas
var_cat = ['type1']
var_num = ['hp','attack','defense','speed','sp_attack','sp_defense','capture_rate','height_m','weight_kg']
modelling_vars = var_num + var_cat

# Agora vamos definir quais serão as features(X) e a target(y)
X = df_status[modelling_vars].copy()
y = df_status['is_legendary'].copy()

X['d_psychic'] = np.where(X['type1']=='psychic',1,0)

X.drop('type1',axis=1,inplace=True)
# Pegaremos a coluna type1 (categórica) e criaremos colunas "dummies" (contendo 0 ou 1 pra cada categoria em "type1")
# Obs: removemos uma das colunas por questões de modelagem onde todas as variáveis (colunas) da base precisam ser *linearmente independentes*
# X = pd.get_dummies(X,prefix='d',columns=['type1']).drop('d_psychic',axis=1)

# A biblioteca do scikit learn tem a classe para realizar a separação da base em treino e test
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,stratify=y,random_state=42)

print(X_train.shape, X_test.shape, y_train.shape, y_test.shape) # Apenas para visualização

# Vamos definir uma variável para armazenar as métricas de cada modelo:
from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score,roc_auc_score

score_list = ['accuracy','precision','recall','f1','roc_auc']

metrics = {
    'model'    : [],
    'accuracy' : [],
    'precision': [],
    'recall'   : [],
    'f1-score' : [],
    'AUC'      : []
}

# Definindo uma função pra incluir as métricas no dicionário

def save_metrics(y_test,y_pred,y_prob,model_name):

    metrics['model'].append(model_name)
    metrics['accuracy'].append(accuracy_score(y_test, y_pred))
    metrics['recall'].append(recall_score(y_test, y_pred))
    metrics['precision'].append(precision_score(y_test, y_pred))
    metrics['f1-score'].append(f1_score(y_test, y_pred))
    metrics['AUC'].append(roc_auc_score(y_test, y_prob[:,1]))

# Função pra limpar os valores do dicionário
# Primeiro parâmetro é o dicionário em si e o segundo é quantos últimos valores devem ser apagados (função .pop())

def pop_values_dict(dict_data,n_last_values=1):

    while n_last_values > 0:   

        for _,v in enumerate(dict_data.items()): 
            
            dict_data[v[0]].pop()
        
        n_last_values -= 1

"""# Treinando e testando os modelos de classificação

### Regressão Logística
"""

from sklearn.linear_model import LogisticRegression

lr = LogisticRegression(random_state=42,max_iter=1000)

lr.fit(X_train,y_train)

y_pred = lr.predict(X_test)
y_prob = lr.predict_proba(X_test)

save_metrics(y_test,y_pred,y_prob,'Logistic Regression')

"""### SVM"""

from sklearn.svm import SVC

svm = SVC(probability=True,random_state=42)

svm.fit(X_train,y_train)

y_pred = svm.predict(X_test)
y_prob = svm.predict_proba(X_test)

save_metrics(y_test,y_pred,y_prob,'SVM')

"""### Decision Tree"""

from sklearn.tree import DecisionTreeClassifier

dt = DecisionTreeClassifier(random_state=42)

dt.fit(X_train,y_train)

y_pred = dt.predict(X_test)
y_prob = dt.predict_proba(X_test)

save_metrics(y_test,y_pred,y_prob,'Decision Tree')

"""### Random Forest"""

from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(oob_score=True,n_jobs=-1,random_state=42)

rf.fit(X_train,y_train)

y_pred = rf.predict(X_test)
y_prob = rf.predict_proba(X_test)

save_metrics(y_test,y_pred,y_prob,'Random Forest')

"""### AdaBoost"""

from sklearn.ensemble import AdaBoostClassifier

ada = AdaBoostClassifier(random_state=42)

ada.fit(X_train,y_train)

y_pred = ada.predict(X_test)
y_prob = ada.predict_proba(X_test)

save_metrics(y_test,y_pred,y_prob,'AdaBoost')

"""### Gradient Boosting"""

from sklearn.ensemble import GradientBoostingClassifier

gb = GradientBoostingClassifier(random_state=42)

gb.fit(X_train,y_train)

y_pred = gb.predict(X_test)
y_prob = gb.predict_proba(X_test)

save_metrics(y_test,y_pred,y_prob,'Gradient Boosting')

"""### XGBoosting"""

#!pip install feature-engine lightgbm xgboost

from xgboost import XGBClassifier

xgb = XGBClassifier(random_state=42)

xgb.fit(X_train,y_train)

y_pred = xgb.predict(X_test)
y_prob = xgb.predict_proba(X_test)

save_metrics(y_test,y_pred,y_prob,'XGBoosting')

"""### LightGBM"""

from lightgbm import LGBMClassifier

lgbm = XGBClassifier(random_state=42)

lgbm.fit(X_train,y_train)

y_pred = lgbm.predict(X_test)
y_prob = lgbm.predict_proba(X_test)

save_metrics(y_test,y_pred,y_prob,'LightGBM')

"""## Analisando as métricas + Gráficos

### Comparando modelos
"""

# Resumo das métricas obtidas dos nossos modelos
pd.DataFrame(metrics)

"""### Cross Validation"""

# Um ponto de atenção: "Como ter mais certeza de que estes números são coerentes ?
# Como saber se, ao pegar 20% da base como teste, teríamos o mesmo resultado se pegássemos outros 20% da mesma base ?
# Teríamos que fazer a separação novamente entre treino e teste e selecionar outros pedaços da base pra garantir
# Por isso tem um conjunto de técnicas de CROSS - VALIDATION pra fazer justamente isso
# Vou utilizar apenas uma mas há outras na documentação do scikit learn

# https://scikit-learn.org/stable/modules/cross_validation.html

# Primeiro, vamos limpar o dicionário para incluir novas métricas

pop_values_dict(metrics,len(metrics['AUC']))

from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import cross_validate

# Instanciando o método de cross validation
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Definindo em algumas métricas que vamos trazer no cross validation
score_list = ['accuracy','precision','recall','f1','roc_auc']

# retorna estatísticas de tempo e métricas de avalição do modelo
cv_results = cross_validate(lr,X,y,scoring=score_list,cv=skf,n_jobs=-1)

cv_results_df = pd.DataFrame(cv_results)

cv_results_df

# Construí uma função pra facilitar o preenchimento das métricas dentro do dicionário depois do cross validation

def save_metrics_crossval(model_instance,scores_list,model_name):

    cv_results = cross_validate(model_instance,
                                X,
                                y,
                                scoring=scores_list,
                                cv=skf,
                                n_jobs=-1)

    cv_results_df = pd.DataFrame(cv_results).mean().round(5)

    metrics['model'].append(model_name)
    metrics['accuracy'].append(cv_results_df['test_accuracy'])
    metrics['precision'].append(cv_results_df['test_precision'])
    metrics['recall'].append(cv_results_df['test_recall'])
    metrics['f1-score'].append(cv_results_df['test_f1'])
    metrics['AUC'].append(cv_results_df['test_roc_auc'])

# Salvando todas as métricas de cada modelo criado anteriormente

save_metrics_crossval(lr,  score_list,'Logistic Regression')
save_metrics_crossval(svm, score_list,'SVM')
save_metrics_crossval(dt,  score_list,'Decision Tree')
save_metrics_crossval(rf,  score_list,'Random Forest')
save_metrics_crossval(ada, score_list,'AdaBoost')
save_metrics_crossval(gb,  score_list,'Gradient Boost')
save_metrics_crossval(xgb, score_list,'XGBoosting')
save_metrics_crossval(lgbm,score_list,'LightGBM')

pd.DataFrame(metrics)

"""### Análises mais avançadas dos modelos

1. Matriz de Confusão (scikit learn):

2. Curva de Ganhos Acumulados (scikitplot):

3. Curva ROC (scikitplot):
"""

# Plotagem da matriz de confusão (ou matriz de classificação) para cada modelo

from sklearn.metrics import plot_confusion_matrix

# sharex e sharey fazem com que os nomes dos eixos entre os gráficos não se repitam, deixando a visão menos poluída.
fig,ax = plt.subplots(2,4,sharex='col', sharey='row',figsize=(20,10))

# Paleta de cores extraída da documentação do Matplotlib
cmaps = ['Greys', 'Purples', 'Blues', 'Greens', 'Oranges', 'Reds',
            'YlOrBr', 'YlOrRd', 'OrRd', 'PuRd', 'RdPu', 'BuPu',
            'GnBu', 'PuBu', 'YlGnBu', 'PuBuGn', 'BuGn', 'YlGn']

cmaps_plot = 'Blues'

# Plotagem
plot_confusion_matrix(lr,  X_test,y_test,display_labels=['Not Legendary','Legendary'],normalize='true',cmap=cmaps_plot,ax=ax[0,0])
plot_confusion_matrix(svm, X_test,y_test,display_labels=['Not Legendary','Legendary'],normalize='true',cmap=cmaps_plot,ax=ax[0,1])
plot_confusion_matrix(dt,  X_test,y_test,display_labels=['Not Legendary','Legendary'],normalize='true',cmap=cmaps_plot,ax=ax[0,2])
plot_confusion_matrix(rf,  X_test,y_test,display_labels=['Not Legendary','Legendary'],normalize='true',cmap=cmaps_plot,ax=ax[0,3])
plot_confusion_matrix(ada, X_test,y_test,display_labels=['Not Legendary','Legendary'],normalize='true',cmap=cmaps_plot,ax=ax[1,0])
plot_confusion_matrix(gb,  X_test,y_test,display_labels=['Not Legendary','Legendary'],normalize='true',cmap=cmaps_plot,ax=ax[1,1])
plot_confusion_matrix(xgb, X_test,y_test,display_labels=['Not Legendary','Legendary'],normalize='true',cmap=cmaps_plot,ax=ax[1,2])
plot_confusion_matrix(lgbm,X_test,y_test,display_labels=['Not Legendary','Legendary'],normalize='true',cmap=cmaps_plot,ax=ax[1,3]);

!pip install scikit-plot

import scikitplot as skplt # https://scikit-plot.readthedocs.io/en/stable/metrics.html

fig,ax = plt.subplots(2,4, sharex='col', sharey='row',figsize=(20,10))


y_probas = lr.predict_proba(X_test)
skplt.metrics.plot_cumulative_gain(y_test, y_probas,title='LogReg - Cumulative Gains Curve',ax=ax[0,0])

y_probas = svm.predict_proba(X_test)
skplt.metrics.plot_cumulative_gain(y_test, y_probas,title='SVM - Cumulative Gains Curve',ax=ax[0,1])

y_probas = dt.predict_proba(X_test)
skplt.metrics.plot_cumulative_gain(y_test, y_probas,title='DecTree - Cumulative Gains Curve',ax=ax[0,2])

y_probas = rf.predict_proba(X_test)
skplt.metrics.plot_cumulative_gain(y_test, y_probas,title='RandForest - Cumulative Gains Curve',ax=ax[0,3])

y_probas = ada.predict_proba(X_test)
skplt.metrics.plot_cumulative_gain(y_test, y_probas,title='GBoost - Cumulative Gains Curve',ax=ax[1,0])

y_probas = ada.predict_proba(X_test)
skplt.metrics.plot_cumulative_gain(y_test, y_probas,title='ADA - Cumulative Gains Curve',ax=ax[1,1])

y_probas = xgb.predict_proba(X_test)
skplt.metrics.plot_cumulative_gain(y_test, y_probas,title='XGBoost - Cumulative Gains Curve',ax=ax[1,2])

y_probas = lgbm.predict_proba(X_test)
skplt.metrics.plot_cumulative_gain(y_test, y_probas,title='LightGBM - Cumulative Gains Curve',ax=ax[1,3]);

fig,ax = plt.subplots(2,4, sharex='col', sharey='row',figsize=(20,10))

y_probas = lr.predict_proba(X_test)
skplt.metrics.plot_roc(y_test, y_probas,title='LogReg - ROC Curve',ax=ax[0,0])

y_probas = svm.predict_proba(X_test)
skplt.metrics.plot_roc(y_test, y_probas,title='SVM - ROC Curve',ax=ax[0,1])

y_probas = dt.predict_proba(X_test)
skplt.metrics.plot_roc(y_test, y_probas,title='DecTree - ROC Curve',ax=ax[0,2])

y_probas = rf.predict_proba(X_test)
skplt.metrics.plot_roc(y_test, y_probas,title='RandForest - ROC Curve',ax=ax[0,3])

y_probas = ada.predict_proba(X_test)
skplt.metrics.plot_roc(y_test, y_probas,title='GBoost - ROC Curve',ax=ax[1,0])

y_probas = ada.predict_proba(X_test)
skplt.metrics.plot_roc(y_test, y_probas,title='ADA - ROC Curve',ax=ax[1,1])

y_probas = xgb.predict_proba(X_test)
skplt.metrics.plot_roc(y_test, y_probas,title='XGBoost - ROC Curve',ax=ax[1,2])

y_probas = lgbm.predict_proba(X_test)
skplt.metrics.plot_roc(y_test, y_probas,title='LightGBM - ROC Curve',ax=ax[1,3]);

"""# Conclusão e Pontos Importantes

1. De todos os modelos, os que mais se destacaram com base nas métricas foram SVM e Random Forest

2. O vencedor mesmo dependerá de cada caso, o que for mais conveniente pra quem estiver utilizando
    
3. O SVM performou melhor que o Random Forest por uma pequena margem !

4. Existem inúmeras possibilidades de parâmetros para cada modelo e ,portanto, há mais formas de otimizá-los. No nosso caso já obtivemos ótimos resultados sem aplicar novas técnicas.
"""
